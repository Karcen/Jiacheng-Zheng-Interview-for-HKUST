{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3699915d-4a4d-4c76-ad3c-86360ed0eb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Token ''B'' not found in vocab.\n",
      "[[1037, 1040, 1046, 1057, 1055, 1056, 1041, 1040, 1052, 1054, 1051, 1049, 1051, 1056, 1041, 1040, 1037, 1039, 1044, 1045, 1041, 1058, 1041, 1040, 1041, 1060, 1052, 1041, 1054, 1045, 1041, 1050, 1039, 1041, 1040], [1059, 1051, 1054, 1047, 1055, 1039, 1044, 1051, 1051, 1048, 1039, 1051, 1057, 1054, 1056, 1052, 1054, 1041, 1055, 1055], [1042, 1037, 1058, 1051, 1054, 1041, 1040, 1056, 1051, 1048, 1041, 1054, 1037, 1056, 1041, 1040, 1056, 1054, 1057, 1055, 1056, 1041, 1040, 1037, 1039, 1047, 1050, 1051, 1059, 1048, 1041, 1040, 1043, 1041, 1040], [1043, 1037, 1054, 1040, 1041, 1050, 1045, 1050, 1043, 1044, 1051, 1049, 1041, 1059, 1051, 1054, 1047, 1038, 1057, 1055, 1045, 1050, 1041, 1055, 1055, 1044, 1051, 1057, 1055, 1041, 1059, 1051, 1054, 1047], [1041, 1060, 1041, 1054, 1039, 1045, 1055, 1041, 1055, 1056, 1057, 1040, 1061, 1041, 1060, 1052, 1048, 1051, 1054, 1041, 1056, 1041, 1037, 1039, 1044], [1042, 1051, 1051, 1040, 1043, 1057, 1041, 1055, 1056, 1055, 1048, 1041, 1055, 1055, 1051, 1050, 1055, 1056, 1041, 1037], [1039, 1048, 1051, 1055, 1041, 1048, 1061, 1040, 1045, 1054, 1041, 1039, 1056, 1048, 1061, 1050, 1041, 1054, 1058, 1051, 1057, 1055, 1048, 1061, 1042, 1054, 1041, 1041, 1048, 1061], [1052, 1054, 1051, 1042, 1041, 1055, 1055, 1045, 1051, 1050, 1037, 1048, 1037, 1059, 1047, 1059, 1037, 1054, 1040, 1055, 1045, 1049, 1052, 1048, 1041, 1052, 1054, 1037, 1039, 1056, 1045, 1039, 1037, 1048], [1049, 1037, 1054, 1047, 1041, 1056, 1049, 1051, 1057, 1050, 1056, 1037, 1045, 1050, 1038, 1041, 1037, 1039, 1044, 1039, 1048, 1037, 1055, 1055], [1055, 1041, 1039, 1054, 1041, 1056, 1038, 1054, 1041, 1037, 1056, 1044, 1058, 1045, 1041, 1059, 1056, 1051, 1050, 1043, 1057, 1041], [1037, 1040, 1049, 1045, 1054, 1041, 1050, 1051, 1056, 1045, 1039, 1041, 1037, 1040, 1051, 1052, 1056, 1058, 1037, 1048, 1057, 1041], [1044, 1037, 1050, 1040, 1055, 1049, 1051, 1057, 1056, 1044, 1041, 1061, 1041, 1055, 1037, 1054, 1049, 1055], [1040, 1045, 1042, 1042, 1045, 1039, 1057, 1048, 1056, 1039, 1051, 1049, 1052, 1048, 1041, 1060, 1044, 1037, 1052, 1052, 1061, 1055, 1045, 1049, 1045, 1048, 1037, 1054], [1043, 1054, 1037, 1056, 1041, 1042, 1057, 1048, 1055, 1057, 1054, 1052, 1054, 1045, 1055, 1041, 1040, 1039, 1051, 1050, 1058, 1045, 1050, 1039, 1041, 1040, 1054, 1041, 1043, 1054, 1041, 1056, 1042, 1057, 1048], [1054, 1041, 1042, 1048, 1041, 1039, 1056, [1038], 1056, 1044, 1054, 1051, 1057, 1043, 1044, 1037, 1055, 1047], [1057, 1050, 1041, 1049, 1052, 1048, 1051, 1061, 1049, 1041, 1050, 1056, 1044, 1041, 1037, 1048, 1056, 1044, 1041, 1040, 1057, 1039, 1037, 1056, 1045, 1051, 1050, 1039, 1051, 1049, 1049, 1057, 1050, 1045, 1039, 1037, 1056, 1045, 1051, 1050], [1037, 1056, 1056, 1041, 1050, 1040, 1054, 1041, 1042, 1041, 1054, 1048, 1041, 1037, 1040, 1056, 1037, 1048, 1047]]\n",
      "[[tensor(5.8528, grad_fn=<DivBackward0>), tensor(-0.0488, grad_fn=<DivBackward0>), tensor(-7.9502, grad_fn=<DivBackward0>), tensor(-2.4810, grad_fn=<DivBackward0>)], [tensor(-1.6529, grad_fn=<DivBackward0>), tensor(-2.1405, grad_fn=<DivBackward0>), tensor(-4.1861, grad_fn=<DivBackward0>), tensor(-3.4076, grad_fn=<DivBackward0>)], [tensor(-0.2393, grad_fn=<DivBackward0>), tensor(6.8006, grad_fn=<DivBackward0>), tensor(-7.9999, grad_fn=<DivBackward0>), tensor(-4.7011, grad_fn=<DivBackward0>)], [tensor(-3.2472, grad_fn=<DivBackward0>), tensor(3.4817, grad_fn=<DivBackward0>), tensor(-3.4199, grad_fn=<DivBackward0>), tensor(-2.5089, grad_fn=<DivBackward0>)], [tensor(-4.8512, grad_fn=<DivBackward0>), tensor(-4.5333, grad_fn=<DivBackward0>), tensor(-4.8512, grad_fn=<DivBackward0>), tensor(-5.7749, grad_fn=<DivBackward0>)], [tensor(-2.5712, grad_fn=<DivBackward0>), tensor(-3.4597, grad_fn=<DivBackward0>), tensor(-3.4597, grad_fn=<DivBackward0>), tensor(-0.9087, grad_fn=<DivBackward0>)], [tensor(-3.1669, grad_fn=<DivBackward0>), tensor(-2.3061, grad_fn=<DivBackward0>), tensor(-3.6862, grad_fn=<DivBackward0>), tensor(-1.5046, grad_fn=<DivBackward0>)], [tensor(-1.1473, grad_fn=<DivBackward0>), tensor(-2.4315, grad_fn=<DivBackward0>), tensor(-0.8501, grad_fn=<DivBackward0>), tensor(-1.8483, grad_fn=<DivBackward0>)], [tensor(-1.9575, grad_fn=<DivBackward0>), tensor(2.1063, grad_fn=<DivBackward0>), tensor(-4.8470, grad_fn=<DivBackward0>), tensor(-2.2226, grad_fn=<DivBackward0>)], [tensor(0.3889, grad_fn=<DivBackward0>), tensor(-4.1091, grad_fn=<DivBackward0>), tensor(-3.0497, grad_fn=<DivBackward0>), tensor(-3.2818, grad_fn=<DivBackward0>)], [tensor(1.4698, grad_fn=<DivBackward0>), tensor(-0.1017, grad_fn=<DivBackward0>), tensor(2.3814, grad_fn=<DivBackward0>), tensor(-0.3020, grad_fn=<DivBackward0>)], [tensor(-1.5157, grad_fn=<DivBackward0>), tensor(1.3346, grad_fn=<DivBackward0>), tensor(-0.9935, grad_fn=<DivBackward0>), tensor(-0.6623, grad_fn=<DivBackward0>)], [tensor(-4.8885, grad_fn=<DivBackward0>), tensor(-0.5330, grad_fn=<DivBackward0>), tensor(-2.6031, grad_fn=<DivBackward0>), tensor(-2.6031, grad_fn=<DivBackward0>)], [tensor(-6.3295, grad_fn=<DivBackward0>), tensor(-6.5860, grad_fn=<DivBackward0>), tensor(4.2690, grad_fn=<DivBackward0>), tensor(-3.0097, grad_fn=<DivBackward0>)], [tensor(-3.3680, grad_fn=<DivBackward0>), tensor(-2.4513, grad_fn=<DivBackward0>), tensor(-5.7217, grad_fn=<DivBackward0>), tensor(-6.3326, grad_fn=<DivBackward0>)], [tensor(-1.7955, grad_fn=<DivBackward0>), tensor(-3.7264, grad_fn=<DivBackward0>), tensor(-2.0915, grad_fn=<DivBackward0>), tensor(-0.1079, grad_fn=<DivBackward0>)], [tensor(1.1402, grad_fn=<DivBackward0>), tensor(-4.8275, grad_fn=<DivBackward0>), tensor(-4.8275, grad_fn=<DivBackward0>), tensor(-0.4125, grad_fn=<DivBackward0>)]]\n",
      "['A', 'A', 'B', 'B', 'B', 'D', 'D', 'C', 'B', 'A', 'C', 'B', 'B', 'C', 'B', 'D', 'A']\n",
      "the correct rate is :11.76470588235294%\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# # from transformers import BertTokenizer, BertForMaskedLM\n",
    "# from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM\n",
    "# import re\n",
    "# from random import *\n",
    "\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')      #导入词库\n",
    "# bert = BertForMaskedLM.from_pretrained('bert-base-uncased')       #导入模型\n",
    "\n",
    "\n",
    "# bert.eval()\n",
    "# # bert.to('cuda:0')       #移动到GPU，没有GPU就不要这行代码了\n",
    "\n",
    "\n",
    "# def ques_proce(file):\n",
    "#     f = open(file, 'r', encoding='gb18030', errors='ignore')\n",
    "#     buffer = f.readline()\n",
    "#     choices=[]\n",
    "#     while buffer!='':\n",
    "#         list=buffer.split()\n",
    "#         one_que=[list[idx] for idx in [2,4,6,8]]\n",
    "#         choices.append(one_que)\n",
    "#         buffer=f.readline()\n",
    "#     return choices\n",
    "    \n",
    "# #选项提取\n",
    "# choices=ques_proce('question.txt')          #处理问题（选项）文本\n",
    "# choices_idx=[]\n",
    "# for choice in choices:          #进行tokenize化\n",
    "#     choice_idx=tokenizer.convert_tokens_to_ids(choice)\n",
    "#     choices_idx.append(choice_idx)\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "# 加载 BERT Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def ques_proce(file):\n",
    "    f = open(file, 'r', encoding='gb18030', errors='ignore')\n",
    "    buffer = f.readline()\n",
    "    choices = []\n",
    "    while buffer != '':\n",
    "        list = buffer.strip().split()\n",
    "        if len(list) >= 9:\n",
    "            one_que = [list[idx] for idx in [2, 4, 6, 8]]\n",
    "            choices.append(one_que)\n",
    "        else:\n",
    "            print(f\"Skipping incomplete line: {buffer.strip()}\")\n",
    "        buffer = f.readline()\n",
    "\n",
    "    return choices\n",
    "\n",
    "# 清理选项，确保没有点号\n",
    "def clean_choices(choices):\n",
    "    cleaned_choices = []\n",
    "    for choice in choices:\n",
    "        cleaned_choice = [item.rstrip('.') for item in choice]\n",
    "        cleaned_choices.append(cleaned_choice)\n",
    "    return cleaned_choices\n",
    "\n",
    "# Tokenizing step with fallback for out of vocabulary words\n",
    "def safe_tokenize(choice, tokenizer):\n",
    "    try:\n",
    "        # 尝试直接转换为ID\n",
    "        return tokenizer.convert_tokens_to_ids(choice)\n",
    "    except KeyError as e:\n",
    "        print(f\"Warning: Token '{e}' not found in vocab.\")\n",
    "        # 对每个词进行 tokenization，拆分词\n",
    "        return [tokenizer.convert_tokens_to_ids(token) for token in tokenizer.tokenize(choice)]\n",
    "\n",
    "# 主代码执行部分\n",
    "choices = ques_proce('question.txt')\n",
    "cleaned_choices = clean_choices(choices)\n",
    "\n",
    "choices_idx = []\n",
    "for choice in cleaned_choices:\n",
    "    choice_idx = []\n",
    "    for token in choice:\n",
    "        # Tokenize the choice one token at a time\n",
    "        token_ids = safe_tokenize(token, tokenizer)\n",
    "        choice_idx.extend(token_ids)  # 把所有 token 的 ID 连接起来\n",
    "    choices_idx.append(choice_idx)\n",
    "\n",
    "print(choices_idx)\n",
    "\n",
    "\n",
    "#建立预测概率矩阵\n",
    "ans_prob=[]\n",
    "for i in range(len(choices)):\n",
    "    ans_prob.append([0.0,0.0,0.0,0.0])\n",
    "\n",
    "def sen2maskIdx(sen_list):\n",
    "    now_mask=0\n",
    "    result=[]\n",
    "    for idx in range(len(sen_list)):\n",
    "        sen=sen_list[idx]\n",
    "        mask_num=sen.count('[MASK]')\n",
    "        maskIdx=[i+now_mask for i in range(mask_num)]\n",
    "        now_mask+=mask_num\n",
    "        result.append(maskIdx)\n",
    "    return result\n",
    " \n",
    " \n",
    " \n",
    "def pass_proce(file, per_times):\n",
    "    \"\"\"\n",
    "    处理文本文件，将含有 [MASK] 的句子与其他句子组合生成新的句子对。\n",
    "    \n",
    "    参数：\n",
    "        file (str): 文本文件路径。\n",
    "        per_times (int): 每个句子需要与多少个不同的句子组合。\n",
    "    \n",
    "    返回：\n",
    "        list: 每个句子组合生成的句子对和掩码索引。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 读取和处理文本内容\n",
    "        with open(file, 'r', encoding='gb18030', errors='ignore') as f:\n",
    "            buffer = f.read()\n",
    "        \n",
    "        buffer = re.sub(r'\\n', ' ', buffer)  # 替换换行符为空格\n",
    "        buffer = re.sub(r'\\(\\d{1,2}\\)_{1,9}', '[MASK]', buffer)  # 替换特定模式为 [MASK]\n",
    "        sen_list = buffer.split('.')  # 以句号分割成句子列表\n",
    "\n",
    "        # 确保句子列表非空\n",
    "        if not sen_list or all(len(sen.strip()) == 0 for sen in sen_list):\n",
    "            raise ValueError(\"文本内容为空，无法生成有效句子。\")\n",
    "\n",
    "        for_all_mask = []\n",
    "        sen2maskidx = sen2maskIdx(sen_list)  # 假定该函数正确生成句子到掩码索引的映射\n",
    "\n",
    "        # 遍历句子\n",
    "        for sen in sen_list:\n",
    "            if '[MASK]' in sen:  # 只处理包含 [MASK] 的句子\n",
    "                for_this_mask = []\n",
    "\n",
    "                for i in range(per_times):\n",
    "                    ano_idx = randint(0, len(sen_list) - 1)\n",
    "\n",
    "                    # 确保选择的句子与当前句子不重复\n",
    "                    while sen_list[ano_idx] == sen:\n",
    "                        ano_idx = randint(0, len(sen_list) - 1)\n",
    "\n",
    "                    # 根据索引顺序构造句子对和掩码索引\n",
    "                    if sen_list.index(sen) > ano_idx:\n",
    "                        temp_sen = '[CLS]' + sen_list[ano_idx].strip() + ' [SEP] ' + sen.strip() + ' [SEP]'\n",
    "                        mask_idx = sen2maskidx[ano_idx] + sen2maskidx[sen_list.index(sen)]\n",
    "                    else:\n",
    "                        temp_sen = '[CLS]' + sen.strip() + ' [SEP] ' + sen_list[ano_idx].strip() + ' [SEP]'\n",
    "                        mask_idx = sen2maskidx[sen_list.index(sen)] + sen2maskidx[ano_idx]\n",
    "\n",
    "                    for_this_mask.append((temp_sen, mask_idx))  # 添加生成的句子对和掩码索引\n",
    "\n",
    "                for_all_mask.append(for_this_mask)\n",
    "\n",
    "        return for_all_mask\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"无法找到文件: {file}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"处理文件时出错: {str(e)}\")\n",
    "        \n",
    "#文本处理\n",
    "text=pass_proce(\"passage.txt\",17)   #处理文章文本\n",
    "for mask_sen in text:\n",
    "    for per_sen in mask_sen:\n",
    "        tokenized_text = tokenizer.tokenize(per_sen[0])\n",
    "        broke_point=tokenized_text.index('[SEP]')\n",
    "        segments_ids=[0]*(broke_point+1)+[1]*(len(tokenized_text)-broke_point-1)\n",
    "        que_idxs=per_sen[1]\n",
    " \n",
    "        ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_text)])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_text)])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "         \n",
    "        #mask的位置提取\n",
    "        mask_num=tokenized_text.count('[MASK]')\n",
    "        mask_idxs=[idx for idx in range(len(tokenized_text)) if tokenized_text[idx]=='[MASK]']\n",
    " \n",
    " \n",
    "        #预测答案\n",
    "        result = bert(ids,segments_tensors)\n",
    "        for i in range(mask_num):\n",
    "            mask_idx=mask_idxs[i]\n",
    "            this_ans_prob = [result[0][mask_idx][choice_idx] for choice_idx in choices_idx[que_idxs[i]]]\n",
    "            ans_prob[que_idxs[i]]=[ans_prob[que_idxs[i]][j]+this_ans_prob[j] for j in range(4)]\n",
    "\n",
    "#归一化\n",
    "for i in range(len(choices)):\n",
    "    for j in range(4):\n",
    "        ans_prob[i][j]/=17\n",
    "\n",
    "#计算预测答案\n",
    "\n",
    "print(ans_prob)\n",
    "\n",
    "ans_pred = []\n",
    "for per_que in ans_prob:\n",
    "    # 确保per_que被正确处理\n",
    "    if isinstance(per_que, torch.Tensor):\n",
    "        # 先detach，确保与计算图分离，再转为CPU的NumPy数组\n",
    "        per_que = per_que.detach().cpu().numpy()\n",
    "    elif isinstance(per_que, list):\n",
    "        # 如果是列表，直接转为NumPy数组\n",
    "        per_que = np.array([pq.detach().cpu().numpy() if isinstance(pq, torch.Tensor) else pq for pq in per_que])\n",
    "\n",
    "    # 获取最大值的索引\n",
    "    ans = ['A', 'B', 'C', 'D'][per_que.argmax(axis=0)]\n",
    "    ans_pred.append(ans)\n",
    "\n",
    "print(ans_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ans_proce(file):\n",
    "    f = open(file, 'r', encoding='gb18030', errors='ignore')\n",
    "    buffer = f.readline()\n",
    "    answers = []\n",
    "    while buffer!='':\n",
    "        buffer=buffer.strip()\n",
    "        answers.append(buffer)\n",
    "        buffer=f.readline()\n",
    "    # print(answers)\n",
    "    return answers\n",
    " \n",
    " \n",
    "#导入正确答案\n",
    "ans_conrrect=ans_proce('answer.txt')\n",
    " \n",
    " \n",
    " \n",
    "#计算正确率\n",
    "correct=0.0\n",
    "for i in range(len(choices)):\n",
    "    if ans_pred[i]==ans_conrrect[i]:\n",
    "        correct+=1\n",
    "print(\"the correct rate is :\"+str(correct/len(choices)*100.0)+\"%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
